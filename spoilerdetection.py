# -*- coding: utf-8 -*-
"""SpoilerDetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sW9F38MjQUYsScXonlmoDfaX2dt8_PX2
"""

import pandas as pd
import numpy as np
!pip install pymongo
import pymongo

import re,string,unicodedata
from bs4 import BeautifulSoup
import nltk
nltk.download('stopwords')
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Dense, Dropout, Flatten

client = pymongo.MongoClient("mongodb+srv://kunal:kunal@sandbox.hjhsy.mongodb.net/?retryWrites=true&w=majority")
db = client['project']
spoiler = list(db['reviews'].find({'is_spoiler': True}, {'review_text':1, 'is_spoiler':1}).limit(40000))
data = spoiler + list(db['reviews'].find({'is_spoiler': False}, {'review_text':1, 'is_spoiler':1}).limit(40000))

imdb = pd.DataFrame(data)
imdb.is_spoiler.value_counts()

def strip_html(text):
  soup = BeautifulSoup(text, 'html.parser')
  return soup.get_text()

def remove_square_brackets(text):
  return re.sub('\[[^]]*\]', '', text)

def remove_special_characters(text):
  pattern = r'[^a-zA-z0-9\s]'
  text = re.sub(pattern, '', text)
  return text

def denoise_text(text):
  text = strip_html(text)
  text = remove_square_brackets(text)
  text = remove_special_characters(text)
  return text

imdb.review_text = imdb.review_text.apply(denoise_text)

def stemmer(text):
  ps = nltk.porter.PorterStemmer()
  text = ' '.join([ps.stem(word) for word in text.split()])
  return text

imdb.review_text = imdb.review_text.apply(stemmer)

from nltk.tokenize.toktok import ToktokTokenizer

stopword_list = nltk.corpus.stopwords.words('english')

def remove_stopwords(text):
  tokens = ToktokTokenizer().tokenize(text)
  tokens = [token.strip() for token in tokens]

  filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
  return ' '.join(filtered_tokens)

imdb.review_text = imdb.review_text.apply(remove_stopwords)

MAX_VOCABULARY = 20000
PADDING = 1000

tokenizer = Tokenizer(num_words=MAX_VOCABULARY)
tokenizer.fit_on_texts(imdb.review_text)

imdb.review_text = tokenizer.texts_to_sequences(imdb.review_text)
imdb.is_spoiler = imdb.is_spoiler.astype(int)

# imdb.review_text = pad_sequences(imdb.review_text, maxlen=PADDING, padding='post')
x_train, x_test, y_train, y_test = train_test_split(imdb.review_text, imdb.is_spoiler, test_size=0.3)

x_train = pad_sequences(x_train, maxlen=PADDING, padding='post')
x_test = pad_sequences(x_test, maxlen=PADDING, padding='post')

print("After padding the training reviews are: ")

for i in range(10):
  print("Review["+ str(i) +"]: ", len(x_train[i]))

word_counts = tokenizer.word_counts

# Sort the word counts in descending order and keep the top 5000 words
top_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:MAX_VOCABULARY]

# Create a dictionary to store the word index mappings
word_index = {}

# Assign indices to the top words
for i, (word, count) in enumerate(top_words, start=1):
    print(word, count, i)
    word_index[word] = i

len(word_index)

import pickle

# Save the word index dictionary
with open('word_index_20000.pkl', 'wb') as f:
    pickle.dump(word_index, f)

model = Sequential()
model.add(Embedding(MAX_VOCABULARY, 32, input_length=PADDING))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=2, batch_size=128, validation_data=(x_test, y_test))

x_train

model.evaluate(x_test,y_test)

import matplotlib.pyplot as plt

# Plotting the evaluation metrics
metrics = {'Loss': 0.421555757522583, 'Accuracy': 0.8073333501815796}
plt.bar(metrics.keys(), metrics.values())
plt.ylabel('Value')
plt.title('Model Evaluation')
plt.show()

tokenizer.word_index

model.save('spoiler_detector_20000.h5')

def preprocess(text):
  text = denoise_text(text)
  text = stemmer(text)
  text = remove_stopwords(text)

  return text

for idx in range(10):
  review = input()
  review = preprocess(review)

  print("Length of review is:",len(review))

  review = tokenizer.texts_to_sequences([review])
  pre_review = pad_sequences(review, maxlen=PADDING, padding='pre')
  post_review = pad_sequences(review, maxlen=PADDING, padding='post')
  # print(review[0])

  pre_prediction = model.predict(pre_review)
  post_prediction = model.predict(post_review)

  # if pre_prediction > 0.85:
  #   print("May spoiler")
  # elif pre_prediction > 0.75:
  #   print("Orange")
  # else:
  #   print("Green")
  print("Post:",post_prediction, "Pre:",pre_prediction)
  print()

import pickle

# Save the word index dictionary
with open('word_index_5000.pkl', 'wb') as f:
    pickle.dump(word_index, f)

# Load the word index dictionary
with open('word_index.pkl', 'rb') as f:
    word_index = pickle.load(f)

word_index

from tensorflow.keras.regularizers import l2

model = Sequential()
model.add(Embedding(MAX_VOCABULARY, 32, input_length=PADDING))
model.add(Flatten())
model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))

from tensorflow.keras.datasets import imdb

imdb.get_word_index()

from tensorflow.keras.models import load_model

load_model('')