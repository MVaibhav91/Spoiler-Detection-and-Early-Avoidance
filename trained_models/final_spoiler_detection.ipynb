{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmm_xic0XWEb"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential , load_model\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZexfutLXbOz",
        "outputId": "b703c30a-0e37-42ed-dee4-f1aff91c0cb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load the IMDB dataset and get the word index\n",
        "num_words = 20000\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "word_index = imdb.get_word_index()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlXXfxgSXcf6"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data\n",
        "maxlen = 10000\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwjCnt02Xfok",
        "outputId": "1beb61cb-7919-4d2d-db97-bec7c8d13db3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 10000, 32)         640000    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 320000)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 320001    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 960,001\n",
            "Trainable params: 960,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_words, 32, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0zOpcJ2Xg3h",
        "outputId": "49ab8488-c73f-4cba-8af3-eeb8183abee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "391/391 [==============================] - 45s 115ms/step - loss: 0.0504 - acc: 0.9874 - val_loss: 0.3356 - val_acc: 0.8780\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 45s 115ms/step - loss: 0.0343 - acc: 0.9930 - val_loss: 0.3432 - val_acc: 0.8786\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 47s 119ms/step - loss: 0.0229 - acc: 0.9959 - val_loss: 0.3607 - val_acc: 0.8768\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 0.0150 - acc: 0.9980 - val_loss: 0.3854 - val_acc: 0.8744\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.0098 - acc: 0.9992 - val_loss: 0.4322 - val_acc: 0.8682\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f04371bb070>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Train the model\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HVfT_-CLXiCz"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save('imdb_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KD__0bobXjK4"
      },
      "outputs": [],
      "source": [
        "# Load the saved model\n",
        "loaded_model = load_model('imdb_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "k0RjSGzWk7Yh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e69960a-9131-4b7f-bb30-1b2e8b9a8305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a movie review to check for spoilers: When I first read \"The Hobbit\" and \"The Lord of the Rings\" Trilogy back in the early 1970's, I recall saying to myself that the story would make a wonderful movie. \"Star Wars\" still hadn't come out yet, and realizing that the available technology could not do justice to the fantastic world presented by J.R.R. Tolkien, it was simply best left to the imagination.  Well, imagination has found life in \"The Fellowship of the Ring\", a truly profound epic that sets the standard for film fantasy, just as the books did for the written word. Upon first seeing it during it's initial release, I couldn't have been happier with the amount of detail it offered while remaining true to the original adventure. Everyone imagines what a story and characters look like in their own mind; it was as if Peter Jackson tapped a great cosmic consciousness to deliver a tale that captured the tone and pacing of the novel dead on.  I feel that readers of the trilogy have a leg up on the characters and locations of Middle Earth, as they are revealed in the film quickly and with nominal explanation. For example, when the Black Riders appear for the first time, it's difficult to grasp what they're all about, other than the fact that they're after the ring. Strider's explanation of the Nazgul is perfect - ring wraiths who were once men, neither alive nor dead, who always feel the power of the ring. Coming to the movie with that understanding ahead of time helps the viewer have a greater appreciation of the action taking place.  The real magic of the movie for me is the seamless manner in which the various races coexist and interact with each other. Though levels of unfamiliarity and distrust appear, can anyone coming out of the movie doubt that elves, dwarfs, hobbits and wizards actually exist. Even orcs and evil Uruk hai have a place in this world, for without the danger they pose there is no triumph.  If the movie captures your imagination and you haven't read the trilogy or it's prequel \"The Hobbit\", you'll be doing yourself a favor to do so. There in even more exquisite detail are nuances such as Elvish poetry and additional characters that provide more depth and color to the world of Middle Earth. It's a world easy to get lost in, and makes one appreciate a writer of legendary proportion who invented a land, people, and language all of his own that can now be shared with everyone.\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "This review contains spoilers.\n"
          ]
        }
      ],
      "source": [
        "# Test the model on a random movie review\n",
        "review = input(\"Enter a movie review to check for spoilers: \")\n",
        "review = review.lower().split()\n",
        "\n",
        "# Replace out-of-vocabulary words with a special token\n",
        "review = [word_index[word] + 3 if word in word_index else 2 for word in review]\n",
        "\n",
        "# Skip words that are not in the word index\n",
        "review = [word for word in review if word < num_words]\n",
        "\n",
        "review = pad_sequences([review], maxlen=maxlen)\n",
        "prediction = loaded_model.predict(review)[0][0]\n",
        "\n",
        "if prediction >= 0.5:\n",
        "    print(\"This review contains spoilers.\")\n",
        "else:\n",
        "    print(\"This review is spoiler-free.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}